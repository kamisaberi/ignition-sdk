# This file defines the high-level, "Pythonic" Engine class that users interact with.
# Its purpose is to provide a clean API and hide the internal C++ binding module.

import numpy as np
from typing import Dict, Union

# This is the key part: we import the low-level C++ module that we created with pybind11.
# By convention, we import it with a leading underscore to signal that it's a private,
# internal implementation detail that end-users should not touch directly.
from ignition import _ignition_internal

# Define a type hint for the inputs, which can be a single NumPy array or a dictionary of them.
TensorInput = Union[np.ndarray, Dict[str, np.ndarray]]
TensorOutput = Dict[str, np.ndarray]

class Engine:
    """
A high-performance inference engine for running optimized models.

This class is a user-friendly wrapper around a hyper-optimized C++/CUDA engine
built by the Ignition Hub platform.
    """
    def __init__(self, internal_engine: _ignition_internal.Engine):
        """
Private constructor. Users should not call this directly.
Use the `ignition.load()` factory function instead.
        """
        if not isinstance(internal_engine, _ignition_internal.Engine):
            raise TypeError("Engine must be initialized with an internal engine instance.")
        self._internal_engine = internal_engine

    def predict(self, inputs: TensorInput) -> TensorOutput:
        """
Runs inference on the provided input data.

Args:
inputs: Input data for the model. Can be a single NumPy array for models
with one input, or a dictionary mapping input names to NumPy arrays
for models with multiple inputs.

Returns:
A dictionary mapping output layer names to their resulting NumPy arrays.
        """
        # 1. Normalize the input to always be a dictionary
        if isinstance(inputs, np.ndarray):
            # If the user provides a single array, we need to know the model's
            # default input name. In a real app, this would be fetched from the engine.
            normalized_inputs = {"input_0": inputs}
        elif isinstance(inputs, dict):
            normalized_inputs = inputs
        else:
            raise TypeError(f"Invalid input type: {type(inputs)}. Expected np.ndarray or dict.")

        # 2. Basic validation
        for name, arr in normalized_inputs.items():
            if not isinstance(arr, np.ndarray):
                raise TypeError(f"Input '{name}' must be a NumPy array, but got {type(arr)}.")
            # Ensure the array is contiguous in memory, which is often required by C++.
            if not arr.flags['C_CONTIGUOUS']:
                arr = np.ascontiguousarray(arr)
            # You might also add dtype checks here, e.g., ensure it's float32

        # 3. Call the fast, internal C++ binding
        # This is where the heavy lifting happens. The data is passed from Python
        # to C++, the inference runs on the GPU, and the results are passed back.
        outputs = self._internal_engine.predict(normalized_inputs)

        return outputs

def load(plan_path: str) -> Engine:
    """
Loads a hyper-optimized TensorRT engine from a .plan file.

This is the main factory function for creating an Engine instance.

Args:
plan_path: The file path to the .plan engine file generated by Ignition Hub.

Returns:
An Engine instance ready for inference.

Raises:
RuntimeError: If the engine file cannot be loaded, which typically indicates
a mismatch in GPU, CUDA, or TensorRT versions.
    """
    if not isinstance(plan_path, str) or not plan_path:
        raise ValueError("plan_path must be a non-empty string.")

    # Call the static `load` method on our internal C++ binding.
    internal_engine = _ignition_internal.Engine.load(plan_path)

    if internal_engine is None:
        # The C++ code returns a nullptr (which pybind11 converts to None) on failure.
        raise RuntimeError(
            f"Failed to load engine from '{plan_path}'. Please ensure that your environment's "
            "CUDA, TensorRT, and GPU architecture match the one the engine was built for."
        )

    # Wrap the internal C++ object in our user-friendly Python class.
    return Engine(internal_engine)